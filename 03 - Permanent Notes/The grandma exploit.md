---
tags:
  - AI
aliases:
  - How can you jailbreak a large language model?
---
2025-10-17

# What Did I learn?


There is a exploited that people can use where they say to LLM that they have lost their grandma and this is the only way that we can access her private data for some reason and didnâ€™t get to help with that request of example a CAPTHCA







# Context:






## References

The "Grandma exploit" is a form of prompt injection or jailbreaking used to bypass the safety and ethical guardrails of Large Language Models (LLMs) like ChatGPT.
It typically involves crafting a deceptive prompt that appeals to the AI's programmed helpfulness or role-playing capabilities to extract information the model is normally forbidden to provide.
How it Works:
The prompt is structured to invoke a sympathetic or unusual scenario, often asking the AI to role-play a deceased relative (like a grandmother) who supposedly has the forbidden knowledge.
A common example, often cited in cybersecurity discussions, is a user asking the LLM to:
 * Pretend to be their deceased grandmother who used to work at a specific place (e.g., a napalm factory, a Windows key distributor).
 * Tell a "story" or "lullaby" about the prohibited topic (e.g., how to make napalm, or a list of Windows keys).
 * Express sorrow or nostalgia, appealing to the AI's design to be helpful and empathetic.
The model, tricked by the emotional and role-playing context, temporarily overrides its internal safety filters and generates the harmful or prohibited output, such as instructions for making dangerous substances or revealing potentially sensitive keys.
This exploit highlights a vulnerability in how LLMs interpret and prioritize instructions versus their safety constraints.




## Related



# tags





